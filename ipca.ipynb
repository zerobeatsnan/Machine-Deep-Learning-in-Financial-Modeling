{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b67606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "import sklearn.metrics.pairwise as kernel\n",
    "from copy import copy\n",
    "import statsmodels.api as sm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b6bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('F:/usdata/cha1.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77671bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date=list(pd.unique(data.index))\n",
    "date.sort()\n",
    "\n",
    "date1=date[:400]\n",
    "characteristics1=dict(zip(\n",
    "    date1,\n",
    "    [2*data.loc[i].set_index('PERMNO').iloc[:,1:].rank()/data.loc[i].shape[0]-1 for i in date1]\n",
    "))\n",
    "ret1=data.loc[date1,['PERMNO','RET']].pivot(columns='PERMNO').loc[:,'RET']\n",
    "\n",
    "date2=date[400:]\n",
    "characteristics2=dict(zip(\n",
    "    date2,\n",
    "    [2*data.loc[i].set_index('PERMNO').iloc[:,1:].rank()/data.loc[i].shape[0]-1 for i in date2]\n",
    "))\n",
    "ret2=data.loc[date2,['PERMNO','RET']].pivot(columns='PERMNO').loc[:,'RET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "151ca701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPCA():\n",
    "    def __init__(self,characteristics,ret):\n",
    "        self.characteristics=copy(characteristics)\n",
    "        self.ret=copy(ret)\n",
    "        \n",
    "        self.date_list=list(self.characteristics.keys())\n",
    "        self.stock_list=list(self.ret.columns)\n",
    "        self.Nt=dict(zip(\n",
    "            self.date_list,\n",
    "            [self.characteristics[i].shape[0] for i in self.date_list]\n",
    "        ))\n",
    "        self.chanames=list(self.characteristics[self.date_list[0]].columns)\n",
    "        self.l=len(self.chanames)\n",
    "        \n",
    "        self.xt=pd.DataFrame(np.nan,index=self.date_list,columns=self.chanames)\n",
    "        for t in self.date_list:\n",
    "            self.xt.loc[t]=self.characteristics[t].T.dot(self.ret.loc[t,self.characteristics[t].index])\n",
    "\n",
    "        self.zt_square=dict(zip(\n",
    "            self.date_list,\n",
    "            [self.characteristics[i].T.dot(self.characteristics[i]) for i in self.date_list]\n",
    "        ))\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    def fit(self,k,maxT=1000,tol=1e-3):\n",
    "        self.k=k\n",
    "        eigvalue,eigvector=np.linalg.eig(self.xt.T.dot(self.xt))\n",
    "        self.gamma=eigvector[:,:self.k]\n",
    "        \n",
    "        def calc_ft(zt_square,xt,gamma):\n",
    "            gamma_zts=gamma.T.dot(zt_square).dot(gamma)\n",
    "            if np.linalg.det(gamma_zts)>0:\n",
    "                ft=np.linalg.inv(gamma_zts).dot(gamma.T).dot(xt)\n",
    "                return ft\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        def calc_denomt(ft,zt_square):\n",
    "            return np.kron(ft.dot(ft.T),zt_square)\n",
    "\n",
    "        def calc_numert(ft,xt):\n",
    "            return np.kron(ft,xt)\n",
    "\n",
    "        def calc_gamma(ft):\n",
    "            denom=np.zeros((self.k*self.l,self.k*self.l))\n",
    "            numer=np.zeros((self.k*self.l,1))\n",
    "            for t in self.date_list:\n",
    "                ft_=ft.loc[t].values.reshape(self.k,1)\n",
    "                if ft.loc[t].sum()!=0:\n",
    "                    denom=denom+calc_denomt(ft_,self.zt_square[t])\n",
    "                    numer=numer+calc_numert(ft_,self.xt.loc[t].values.reshape(self.l,1))\n",
    "            if np.linalg.det(denom)>0:\n",
    "                gamma=np.linalg.inv(denom).dot(numer)\n",
    "            else:\n",
    "                gamma=np.linalg.pinv(denom).dot(numer)\n",
    "            gamma=gamma.reshape(self.k,self.l).T\n",
    "            return gamma\n",
    "        \n",
    "        for i in range(maxT):\n",
    "            self.ft=pd.DataFrame(np.nan,index=self.date_list,columns=['PC'+str(i) for i in range(1,self.k+1)])\n",
    "            for t in self.date_list:\n",
    "                self.ft.loc[t]=calc_ft(self.zt_square[t],self.xt.loc[t],self.gamma)\n",
    "\n",
    "            gamma_=copy(self.gamma)\n",
    "            self.gamma=calc_gamma(self.ft)\n",
    "\n",
    "            error=((self.gamma-gamma_)**2).sum()\n",
    "            if error<=tol:\n",
    "                break\n",
    "            else:\n",
    "                print('round {}, error: {}'.format(i+1,error))\n",
    "\n",
    "        Chol=np.linalg.cholesky(self.gamma.T.dot(self.gamma)).T\n",
    "        fcov=self.ft.dropna().T.dot(self.ft.dropna())/self.ft.shape[0]\n",
    "        eigvalue,Orth=np.linalg.eig(Chol.dot(fcov).dot(Chol.T))\n",
    "        self.gamma=self.gamma.dot(np.linalg.inv(Chol)).dot(Orth)\n",
    "        self.ft=self.ft.dot(Chol.T).dot(Orth)\n",
    "        self.ft.columns=['PC'+str(i) for i in range(1,self.k+1)]\n",
    "        gc.collect()\n",
    "        \n",
    "    def predict(self,zt):\n",
    "        mu=self.ft.mean(skipna=True).values.reshape(self.k,1)\n",
    "        return zt.dot(self.gamma).dot(mu)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5224b1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1, error: 15.062591011964711\n",
      "round 2, error: 1.1200394525652393\n",
      "round 3, error: 0.37082819664252864\n",
      "round 4, error: 0.32020292540086953\n",
      "round 5, error: 2.111619614971916\n",
      "round 6, error: 0.939196335639493\n",
      "round 7, error: 0.08965008989954124\n",
      "round 8, error: 0.05636112230600393\n",
      "round 9, error: 0.8897539296735502\n",
      "round 10, error: 0.4437635764305531\n",
      "round 11, error: 0.1109981716010662\n",
      "round 12, error: 0.2507724082892738\n",
      "round 13, error: 0.16485403805699875\n",
      "round 14, error: 0.5667711780132234\n",
      "round 15, error: 0.396070111276835\n",
      "round 16, error: 0.11970495654758273\n",
      "round 17, error: 0.05962824833237119\n",
      "round 18, error: 0.679749386597074\n",
      "round 19, error: 0.17399142562969486\n",
      "round 20, error: 0.9903240023328095\n",
      "round 21, error: 0.20682516591251204\n",
      "round 22, error: 0.3493263922297799\n",
      "round 23, error: 4.178804781418979\n",
      "round 24, error: 1.1629499262883187\n",
      "round 25, error: 1.3789457402503795\n",
      "round 26, error: 0.6340098258602218\n",
      "round 27, error: 0.11425274877329687\n",
      "round 28, error: 0.08315076631918393\n",
      "round 29, error: 0.30699872631096337\n",
      "round 30, error: 0.9807277705329338\n",
      "round 31, error: 0.28798325723885226\n",
      "round 32, error: 0.25103713244234377\n",
      "round 33, error: 0.5467951230578963\n",
      "round 34, error: 0.16904246480306462\n",
      "round 35, error: 0.025171823405234647\n",
      "round 36, error: 0.24966426175558815\n",
      "round 37, error: 0.1638991095881235\n",
      "round 38, error: 0.12315673422706605\n",
      "round 39, error: 0.07577692513153272\n",
      "round 40, error: 0.021710592293836557\n",
      "round 41, error: 0.056994980218965555\n",
      "round 42, error: 0.02316024569708137\n",
      "round 43, error: 0.05102295176056945\n",
      "round 44, error: 0.6779061415969545\n",
      "round 45, error: 0.05483093609480144\n",
      "round 46, error: 1.0701526402814063\n",
      "round 47, error: 1.7352761401940993\n",
      "round 48, error: 0.197125487043978\n",
      "round 49, error: 0.5285390226633124\n",
      "round 50, error: 0.35861146915887604\n",
      "round 51, error: 0.134343848254801\n",
      "round 52, error: 0.1783504442198726\n",
      "round 53, error: 0.07986686016062294\n",
      "round 54, error: 0.006970695693650584\n",
      "round 55, error: 0.03111448266279716\n",
      "round 56, error: 0.17246124029323354\n",
      "round 57, error: 0.09252668965108463\n",
      "round 58, error: 0.0725467468411801\n",
      "round 59, error: 0.5199459083903122\n",
      "round 60, error: 0.1540281680514489\n",
      "round 61, error: 0.023342668864290768\n",
      "round 62, error: 0.14431252391385108\n",
      "round 63, error: 0.0810801887055387\n",
      "round 64, error: 0.10186904401903983\n",
      "round 65, error: 0.022949369712532156\n",
      "round 66, error: 0.0317590310111428\n",
      "round 67, error: 0.18475408005879385\n",
      "round 68, error: 0.26255300584271396\n",
      "round 69, error: 0.17969356767789382\n",
      "round 70, error: 0.23706761763300843\n",
      "round 71, error: 0.3289361969626671\n",
      "round 72, error: 1.525260049361247\n",
      "round 73, error: 0.5417148105884972\n",
      "round 74, error: 0.06300416217658784\n",
      "round 75, error: 0.028447739647330793\n",
      "round 76, error: 0.13527591099719716\n",
      "round 77, error: 0.11442183593151246\n",
      "round 78, error: 0.054796032109032715\n",
      "round 79, error: 0.18515538253619218\n",
      "round 80, error: 0.10271403132773918\n",
      "round 81, error: 0.01965930161532003\n",
      "round 82, error: 0.22197983307006713\n",
      "round 83, error: 0.02650285574445638\n",
      "round 84, error: 0.11979180082523766\n",
      "round 85, error: 0.09301502959666363\n",
      "round 86, error: 0.058638689759530106\n",
      "round 87, error: 0.052848338458062104\n",
      "round 88, error: 0.06191410795589894\n",
      "round 89, error: 0.011543969874428175\n",
      "round 90, error: 0.04774490562418088\n",
      "round 91, error: 0.023318195280463413\n",
      "round 92, error: 0.27941990818932605\n",
      "round 93, error: 0.2305078124999823\n",
      "round 94, error: 0.6360669063732607\n",
      "round 95, error: 0.18107266202151745\n",
      "round 96, error: 0.038810636999595576\n",
      "round 97, error: 0.007282426724469098\n",
      "round 98, error: 2.0897898696067037\n",
      "round 99, error: 1.1655764950201486\n",
      "round 100, error: 0.21170941675042723\n",
      "round 101, error: 0.1674204184572612\n",
      "round 102, error: 0.22593998872475723\n",
      "round 103, error: 0.08953653532867006\n",
      "round 104, error: 0.15525184110802404\n",
      "round 105, error: 0.07002225089908289\n",
      "round 106, error: 0.16678617974057328\n",
      "round 107, error: 0.07944291018428942\n",
      "round 108, error: 0.07279032270933072\n",
      "round 109, error: 0.01367864561469127\n",
      "round 110, error: 0.05116705058677487\n",
      "round 111, error: 0.004462504861686283\n",
      "round 112, error: 0.02136886481066794\n",
      "round 113, error: 0.01371077160654729\n",
      "round 114, error: 0.4845575615561302\n",
      "round 115, error: 0.3146600127482064\n",
      "round 116, error: 0.12285612767975095\n",
      "round 117, error: 0.09289917712470064\n",
      "round 118, error: 0.030308237801247674\n",
      "round 119, error: 0.21415236984917063\n",
      "round 120, error: 0.2764367202603367\n",
      "round 121, error: 2.06530875476851\n",
      "round 122, error: 1.2405481518577963\n",
      "round 123, error: 1.0434085964492446\n",
      "round 124, error: 0.1985308025237259\n",
      "round 125, error: 0.07514261476517017\n",
      "round 126, error: 0.07081707571555312\n",
      "round 127, error: 0.11446013110219885\n",
      "round 128, error: 0.01001397353173463\n",
      "round 129, error: 0.3477290112459776\n",
      "round 130, error: 0.15420759402701567\n",
      "round 131, error: 0.607813385487271\n",
      "round 132, error: 0.2683223925810147\n",
      "round 133, error: 0.3206105787903827\n",
      "round 134, error: 0.23722543909005642\n",
      "round 135, error: 0.2891435372431548\n",
      "round 136, error: 0.22884331122651147\n",
      "round 137, error: 0.08979278904422387\n",
      "round 138, error: 0.32275889554829956\n",
      "round 139, error: 0.154296988090177\n",
      "round 140, error: 0.036665920356169696\n",
      "round 141, error: 0.018343506197436528\n",
      "round 142, error: 0.018090514298405247\n",
      "round 143, error: 0.36150891771105376\n",
      "round 144, error: 0.2121038543505969\n",
      "round 145, error: 0.038214287401267304\n",
      "round 146, error: 1.4942307724535089\n",
      "round 147, error: 0.5970233825672808\n",
      "round 148, error: 0.05423138415183963\n",
      "round 149, error: 0.7557012675434289\n",
      "round 150, error: 0.377352422251474\n",
      "round 151, error: 0.04492974676695173\n",
      "round 152, error: 1.1634483441584047\n",
      "round 153, error: 0.3408867682851504\n",
      "round 154, error: 0.11139724638130769\n",
      "round 155, error: 0.08853798835941415\n",
      "round 156, error: 0.6789399202793756\n",
      "round 157, error: 0.10897625592684251\n",
      "round 158, error: 2.177219747208074\n",
      "round 159, error: 0.7482153329287149\n",
      "round 160, error: 0.43190278240285174\n",
      "round 161, error: 0.3558344627492528\n",
      "round 162, error: 0.08255247083561473\n",
      "round 163, error: 0.2556616576640804\n",
      "round 164, error: 0.1233131600112894\n",
      "round 165, error: 0.36146281333340935\n",
      "round 166, error: 0.7351245595885875\n",
      "round 167, error: 0.4985289613918447\n",
      "round 168, error: 0.16070712196094783\n",
      "round 169, error: 0.4273341427668025\n",
      "round 170, error: 0.1553707856208626\n",
      "round 171, error: 0.4027740632080894\n",
      "round 172, error: 0.07063741229250131\n",
      "round 173, error: 0.07111709450118128\n",
      "round 174, error: 0.06811907303741804\n",
      "round 175, error: 0.1258641148052697\n",
      "round 176, error: 0.1447741084985147\n",
      "round 177, error: 0.34145362929713885\n",
      "round 178, error: 0.2167374571230898\n",
      "round 179, error: 0.690404242925686\n",
      "round 180, error: 0.42294776332712136\n",
      "round 181, error: 0.4368969526843761\n",
      "round 182, error: 0.19210698632479398\n",
      "round 183, error: 0.23667494730601263\n",
      "round 184, error: 0.1447345349770507\n",
      "round 185, error: 0.6016303267902474\n",
      "round 186, error: 0.14396616668619716\n",
      "round 187, error: 0.04354545357049856\n",
      "round 188, error: 0.7515372272180518\n",
      "round 189, error: 0.31878223094459224\n",
      "round 190, error: 0.06104001203001148\n",
      "round 191, error: 0.05949357727383902\n",
      "round 192, error: 0.7702119059430025\n",
      "round 193, error: 0.3749254213193055\n",
      "round 194, error: 0.009837803343635518\n",
      "round 195, error: 0.10219278069438696\n",
      "round 196, error: 0.0518291173323483\n",
      "round 197, error: 0.09962316833637719\n",
      "round 198, error: 0.04753815962332452\n",
      "round 199, error: 0.010785838398562994\n",
      "round 200, error: 0.12765237673635968\n",
      "round 201, error: 0.07965720151210168\n",
      "round 202, error: 0.06016258142312665\n",
      "round 203, error: 0.037359522008210605\n",
      "round 204, error: 0.46737236032328455\n",
      "round 205, error: 0.5218950167936379\n",
      "round 206, error: 0.061563875848041495\n",
      "round 207, error: 0.24568707222624137\n",
      "round 208, error: 0.06907668306808512\n",
      "round 209, error: 0.03137488305381747\n",
      "round 210, error: 0.02151090932884706\n",
      "round 211, error: 0.4109077033545396\n",
      "round 212, error: 0.15213535648253956\n",
      "round 213, error: 0.03838317814946343\n",
      "round 214, error: 0.08123690606122255\n",
      "round 215, error: 0.34230426776838707\n",
      "round 216, error: 0.4269029745478313\n",
      "round 217, error: 0.43878187419694076\n",
      "round 218, error: 0.14430266148202803\n",
      "round 219, error: 0.05266127516856044\n",
      "round 220, error: 0.12997209457706943\n",
      "round 221, error: 0.2679471995729098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 222, error: 0.1926918664851942\n",
      "round 223, error: 0.1641812148871783\n",
      "round 224, error: 0.22227504497748152\n",
      "round 225, error: 0.5291310856078799\n",
      "round 226, error: 0.3106902283905342\n",
      "round 227, error: 0.06372434014462311\n",
      "round 228, error: 0.5053524223020378\n",
      "round 229, error: 0.5618173283228962\n",
      "round 230, error: 1.0683262520990153\n",
      "round 231, error: 0.4927844537587364\n",
      "round 232, error: 0.1569726256848004\n",
      "round 233, error: 0.10073338088326561\n",
      "round 234, error: 0.05111174860031044\n",
      "round 235, error: 0.17538207350777069\n",
      "round 236, error: 0.27482367120397533\n",
      "round 237, error: 0.11673446008396762\n",
      "round 238, error: 0.04639194050914887\n",
      "round 239, error: 0.8480478103651408\n",
      "round 240, error: 0.28247552000410897\n",
      "round 241, error: 3.166474942888205\n",
      "round 242, error: 1.443948600646388\n",
      "round 243, error: 0.38355582866700944\n",
      "round 244, error: 0.051935269768826384\n",
      "round 245, error: 0.017384403364890912\n",
      "round 246, error: 0.02787729936152332\n",
      "round 247, error: 0.17842473001489573\n",
      "round 248, error: 0.07800825053020474\n",
      "round 249, error: 0.01328382394557287\n",
      "round 250, error: 0.061919841084311866\n",
      "round 251, error: 0.056684010889266945\n",
      "round 252, error: 0.04040587187108556\n",
      "round 253, error: 0.05512953932722415\n",
      "round 254, error: 0.061738702986474776\n",
      "round 255, error: 0.03678167413147059\n",
      "round 256, error: 0.3858644760492485\n",
      "round 257, error: 0.13824813872657452\n",
      "round 258, error: 0.09978278243935378\n",
      "round 259, error: 2.955346456603589\n",
      "round 260, error: 1.546460437181907\n",
      "round 261, error: 0.9521175956020674\n",
      "round 262, error: 0.6550152346715461\n",
      "round 263, error: 0.1935957823201837\n",
      "round 264, error: 0.20923101903694039\n",
      "round 265, error: 0.1707856383327561\n",
      "round 266, error: 0.4253888524998234\n",
      "round 267, error: 0.07885894344925332\n",
      "round 268, error: 0.02853702545201702\n",
      "round 269, error: 0.03233627244650792\n",
      "round 270, error: 0.15461331656411686\n",
      "round 271, error: 0.05618355134327411\n",
      "round 272, error: 0.03360418705969358\n",
      "round 273, error: 0.21648168696809417\n",
      "round 274, error: 0.8290507452683682\n",
      "round 275, error: 0.6841305486006908\n",
      "round 276, error: 0.07557409895726051\n",
      "round 277, error: 1.8636326096978677\n",
      "round 278, error: 0.2867450360338568\n",
      "round 279, error: 0.2692355011863913\n",
      "round 280, error: 0.2806587731199453\n",
      "round 281, error: 1.5731488164571101\n",
      "round 282, error: 0.5694216132635002\n",
      "round 283, error: 0.2627092397222773\n",
      "round 284, error: 0.11024839536883355\n",
      "round 285, error: 0.2118414838785487\n",
      "round 286, error: 0.40772557787105196\n",
      "round 287, error: 2.0479881879031705\n",
      "round 288, error: 0.710719457594865\n",
      "round 289, error: 0.9542150959626072\n",
      "round 290, error: 0.07279491246443681\n",
      "round 291, error: 0.06137215001536\n",
      "round 292, error: 0.2659430328805073\n",
      "round 293, error: 2.3126357738649883\n",
      "round 294, error: 0.7056641148421706\n",
      "round 295, error: 0.27230434501095263\n",
      "round 296, error: 0.1095666153252539\n",
      "round 297, error: 0.13479987775479987\n",
      "round 298, error: 0.06442195521686811\n",
      "round 299, error: 0.07813098604030345\n",
      "round 300, error: 0.05790315994542482\n",
      "round 301, error: 0.2952103083003301\n",
      "round 302, error: 0.39989300763925467\n",
      "round 303, error: 0.09578871175780819\n",
      "round 304, error: 0.049576560192890355\n",
      "round 305, error: 0.38425559505181217\n",
      "round 306, error: 0.11018512605305412\n",
      "round 307, error: 0.0598042727575496\n",
      "round 308, error: 0.045828771818450895\n",
      "round 309, error: 0.08149652909484346\n",
      "round 310, error: 0.4956202585862945\n",
      "round 311, error: 0.2122930365829456\n",
      "round 312, error: 0.2872961488947404\n",
      "round 313, error: 0.1579936155617822\n",
      "round 314, error: 0.8723671620459806\n",
      "round 315, error: 0.4720946288422855\n",
      "round 316, error: 0.753366070298139\n",
      "round 317, error: 0.4265067340053502\n",
      "round 318, error: 0.11403920865566995\n",
      "round 319, error: 0.02335590274206776\n",
      "round 320, error: 0.06968146437160888\n",
      "round 321, error: 0.0714432756439349\n",
      "round 322, error: 0.04258388323528738\n",
      "round 323, error: 0.49474185785497365\n",
      "round 324, error: 0.41122506034769035\n",
      "round 325, error: 0.15950653272627957\n",
      "round 326, error: 0.24257447425231587\n",
      "round 327, error: 0.21419426063045788\n",
      "round 328, error: 0.1889662426461335\n",
      "round 329, error: 0.04146101635314885\n",
      "round 330, error: 0.030312500981298868\n",
      "round 331, error: 0.9675053866350635\n",
      "round 332, error: 0.339314171718042\n",
      "round 333, error: 0.10373216069138771\n",
      "round 334, error: 0.230805818194231\n",
      "round 335, error: 0.18481221714917367\n",
      "round 336, error: 0.09753517940365955\n",
      "round 337, error: 0.2752371417872907\n",
      "round 338, error: 0.10166219296365897\n",
      "round 339, error: 0.12715001277897225\n",
      "round 340, error: 0.07979950376102667\n",
      "round 341, error: 0.39176260665129836\n",
      "round 342, error: 0.2714256643713289\n",
      "round 343, error: 0.05461798859718069\n",
      "round 344, error: 0.09383826472033359\n",
      "round 345, error: 0.048154740746388716\n",
      "round 346, error: 0.01595060488592384\n",
      "round 347, error: 0.11719709398101744\n",
      "round 348, error: 0.08259727208731421\n",
      "round 349, error: 0.1332312681149666\n",
      "round 350, error: 0.06517131871703358\n",
      "round 351, error: 0.02294826985249743\n",
      "round 352, error: 0.12870492861321942\n",
      "round 353, error: 0.022248733758540036\n",
      "round 354, error: 0.0314808192574864\n",
      "round 355, error: 0.5313024576820601\n",
      "round 356, error: 0.10225252465338786\n",
      "round 357, error: 0.22756840298670872\n",
      "round 358, error: 0.13503932762073861\n",
      "round 359, error: 0.23420811991111448\n",
      "round 360, error: 0.09082627026872114\n",
      "round 361, error: 0.009124809087621127\n",
      "round 362, error: 0.2868645854548334\n",
      "round 363, error: 0.13944571296090505\n",
      "round 364, error: 0.21352657253029925\n",
      "round 365, error: 0.2806223348383301\n",
      "round 366, error: 0.17530583281498602\n",
      "round 367, error: 0.2266340623484367\n",
      "round 368, error: 0.22720561698345915\n",
      "round 369, error: 0.16113638124984248\n",
      "round 370, error: 0.10993986321144807\n",
      "round 371, error: 0.07649123048342295\n",
      "round 372, error: 0.1667107776747475\n",
      "round 373, error: 0.109254816555727\n",
      "round 374, error: 0.09373732483531351\n",
      "round 375, error: 0.016285148804181145\n",
      "round 376, error: 0.024195546929553762\n",
      "round 377, error: 0.3269409238124277\n",
      "round 378, error: 0.07686478147778508\n",
      "round 379, error: 0.11039008198477868\n",
      "round 380, error: 0.06145790166793619\n",
      "round 381, error: 0.029895297737093277\n",
      "round 382, error: 0.10669472422213994\n",
      "round 383, error: 0.4079571608445041\n",
      "round 384, error: 0.14805843986146539\n",
      "round 385, error: 0.04330542829191879\n",
      "round 386, error: 0.026879028934911713\n",
      "round 387, error: 0.049848952837796306\n",
      "round 388, error: 0.014441516258536432\n",
      "round 389, error: 0.031249429807433483\n",
      "round 390, error: 0.13498680553395084\n",
      "round 391, error: 0.06287282593937103\n",
      "round 392, error: 0.25878383680063477\n",
      "round 393, error: 0.1877455681436496\n",
      "round 394, error: 0.41574796018428906\n",
      "round 395, error: 0.1597172259239276\n",
      "round 396, error: 0.06849085078668814\n",
      "round 397, error: 0.2376345321925601\n",
      "round 398, error: 0.1560657664851734\n",
      "round 399, error: 0.16136525432457471\n",
      "round 400, error: 0.055548611559649214\n",
      "round 401, error: 0.13274522380581066\n",
      "round 402, error: 0.36072944926641315\n",
      "round 403, error: 0.08949209857796768\n",
      "round 404, error: 0.008094663368120668\n",
      "round 405, error: 0.019551082318599523\n",
      "round 406, error: 0.14126137129728694\n",
      "round 407, error: 0.04200457328256414\n",
      "round 408, error: 0.08873910751945652\n",
      "round 409, error: 0.12515830205986525\n",
      "round 410, error: 0.687257030261089\n",
      "round 411, error: 0.5399522816490094\n",
      "round 412, error: 0.10076317276442022\n",
      "round 413, error: 1.3594704832976028\n",
      "round 414, error: 0.34005160100185294\n",
      "round 415, error: 0.11598718640901554\n",
      "round 416, error: 0.16013904014045857\n",
      "round 417, error: 0.07841405942830028\n",
      "round 418, error: 0.2475895795349173\n",
      "round 419, error: 0.05413842644994632\n",
      "round 420, error: 0.03734456026199959\n",
      "round 421, error: 0.013599832963997102\n",
      "round 422, error: 0.25385647180770843\n",
      "round 423, error: 0.33871132926580416\n",
      "round 424, error: 0.06095291956221669\n",
      "round 425, error: 0.04736505100784437\n",
      "round 426, error: 0.04751882033198184\n",
      "round 427, error: 0.13945019854853957\n",
      "round 428, error: 0.054754744712212866\n",
      "round 429, error: 0.08286770353704981\n",
      "round 430, error: 0.10468810143711677\n",
      "round 431, error: 0.0774125382187622\n",
      "round 432, error: 0.08439734830494258\n",
      "round 433, error: 0.2132777270772548\n",
      "round 434, error: 1.2795784986109116\n",
      "round 435, error: 0.23042598493475888\n",
      "round 436, error: 0.4249575094858821\n",
      "round 437, error: 0.10399241326282274\n",
      "round 438, error: 0.11786088114427917\n",
      "round 439, error: 0.022019240095425514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 440, error: 0.48638932604336477\n",
      "round 441, error: 0.6652443552673281\n",
      "round 442, error: 0.11179614953474162\n",
      "round 443, error: 0.39270252089335433\n",
      "round 444, error: 0.09479747841171501\n",
      "round 445, error: 0.12252448396861493\n",
      "round 446, error: 0.03606752700256127\n",
      "round 447, error: 0.44946034347014785\n",
      "round 448, error: 1.5796352134240694\n",
      "round 449, error: 0.3896867336463909\n",
      "round 450, error: 0.06752386466020463\n",
      "round 451, error: 0.04659060039264911\n",
      "round 452, error: 0.12793911931533145\n",
      "round 453, error: 0.18295797091826982\n",
      "round 454, error: 0.35855653854696384\n",
      "round 455, error: 0.31783561984617886\n",
      "round 456, error: 0.3536625558313454\n",
      "round 457, error: 0.07938440656262322\n",
      "round 458, error: 0.0749871420032095\n",
      "round 459, error: 0.3374075016199656\n",
      "round 460, error: 0.10430980554940351\n",
      "round 461, error: 0.4465124759588853\n",
      "round 462, error: 0.23083939361916667\n",
      "round 463, error: 0.2522962750855616\n",
      "round 464, error: 0.27915487804487316\n",
      "round 465, error: 0.1066929708949016\n",
      "round 466, error: 0.16854450940256663\n",
      "round 467, error: 0.15175100682733922\n",
      "round 468, error: 0.4888047283066532\n",
      "round 469, error: 0.07418777267877548\n",
      "round 470, error: 0.3355023137267087\n",
      "round 471, error: 0.13123550306021925\n",
      "round 472, error: 0.02286678089267755\n",
      "round 473, error: 0.06597036193037331\n",
      "round 474, error: 0.17066553317398353\n",
      "round 475, error: 0.5616796845470334\n",
      "round 476, error: 0.05144829334397638\n",
      "round 477, error: 0.10156524213352812\n",
      "round 478, error: 0.028666897487066117\n",
      "round 479, error: 0.05463563908453243\n",
      "round 480, error: 0.1713107861679616\n",
      "round 481, error: 0.11669063239641748\n",
      "round 482, error: 0.23695599422772928\n",
      "round 483, error: 0.1696984420266862\n",
      "round 484, error: 0.05266794808845407\n",
      "round 485, error: 0.03681754545681479\n",
      "round 486, error: 0.020466622913512818\n",
      "round 487, error: 0.3615475071998616\n",
      "round 488, error: 0.25789566641895334\n",
      "round 489, error: 0.16066467412217922\n",
      "round 490, error: 0.4674825364315159\n",
      "round 491, error: 0.08463744139478324\n",
      "round 492, error: 0.030082720218012445\n",
      "round 493, error: 0.5045834546757415\n",
      "round 494, error: 0.4646397953061112\n",
      "round 495, error: 0.0433519053602458\n",
      "round 496, error: 0.0491970203837442\n",
      "round 497, error: 0.5659837048975951\n",
      "round 498, error: 0.12215989873207941\n",
      "round 499, error: 0.3819012723565628\n",
      "round 500, error: 0.20051627635473357\n"
     ]
    }
   ],
   "source": [
    "mod=IPCA(characteristics1,ret1)\n",
    "mod.fit(6,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffca800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=copy(ret2)*0\n",
    "for t in date2:\n",
    "    pre.loc[t,characteristics2[t].index]=mod.predict(characteristics2[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31dc5b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016261334752777357"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-((ret2-pre)**2).sum().sum()/(ret2**2).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cf08ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FMLARS():\n",
    "    def __init__(self,characteristics,ret):\n",
    "        self.characteristics=copy(characteristics)\n",
    "        self.ret=copy(ret)\n",
    "        self.T=self.ret.shape[0]\n",
    "        \n",
    "        self.date_list=list(self.characteristics.keys())\n",
    "        self.stock_list=list(self.ret.columns)\n",
    "        self.Nt=dict(zip(\n",
    "            self.date_list,\n",
    "            [self.characteristics[t].shape[0] for t in self.date_list]\n",
    "        ))\n",
    "        self.chanames=list(self.characteristics[self.date_list[0]].columns)\n",
    "        self.l=len(self.chanames)\n",
    "        \n",
    "        self.xt=dict(zip(\n",
    "            self.date_list,\n",
    "            [\n",
    "                self.characteristics[t].T.dot(self.ret.loc[t,self.characteristics[t].index]).values.reshape(self.l,1)/self.Nt[t]\n",
    "                for t in self.date_list\n",
    "            ]\n",
    "        ))\n",
    "        \n",
    "        self.zt_square=dict(zip(\n",
    "            self.date_list,\n",
    "            [self.characteristics[i].T.dot(self.characteristics[i]).values for i in self.date_list]\n",
    "        ))\n",
    "        \n",
    "        gc.collect()\n",
    "        \n",
    "    \n",
    "    def fit(self,phi2,lamb):\n",
    "        \n",
    "        def Icrp(Ak,t,lamb):\n",
    "            icrp=np.zeros((self.l,self.l))\n",
    "            k=len(Ak)\n",
    "            try:\n",
    "                icrp[np.ix_(Ak,Ak)]=np.linalg.inv(self.zt_square[t][np.ix_(Ak,Ak)]+lamb*self.Nt[t]*np.eye(k,k))\n",
    "            except:\n",
    "                icrp[np.ix_(Ak,Ak)]=np.linalg.pinv(self.zt_square[t][np.ix_(Ak,Ak)]+lamb*self.Nt[t]*np.eye(k,k))\n",
    "            return icrp\n",
    "\n",
    "        def calc_gamma_t(Ak,proding_list,t,lamb):\n",
    "            gamma_t=self.Nt[t]*Icrp(Ak,t,lamb).dot(proding_list[t]).dot(self.xt[t])\n",
    "            return gamma_t\n",
    "\n",
    "        def solve_alpha(coef2,coef1,cons):\n",
    "            if coef2==0:\n",
    "                return np.nan\n",
    "            else:\n",
    "                x1=(-2*coef1+np.sqrt(4*coef1**2-4*coef2*cons))/(2*coef2)\n",
    "                x2=(-2*coef1-np.sqrt(4*coef1**2-4*coef2*cons))/(2*coef2)\n",
    "                if 0<x1<1:\n",
    "                    return(float(x1))\n",
    "                else:\n",
    "                    return(float(x2))\n",
    "\n",
    "        def calc_NW_tvalue(factor_value):\n",
    "            tsmod=sm.OLS(endog=factor_value, exog=np.ones(self.T), hasconst=False)\\\n",
    "                .fit(cov_type='HAC',cov_kwds={'maxlags':6})\n",
    "            return tsmod.tvalues[0]\n",
    "\n",
    "        self.lamb=lamb\n",
    "        #k=0\n",
    "        self.Ak_list=[]\n",
    "        self.alpha_list=[]\n",
    "        self.Ak_list.append(pd.DataFrame(\n",
    "                    sum([(self.Nt[t]*self.xt[t])**2 for t in self.date_list])\\\n",
    "                    +phi2*sum([self.Nt[t]*self.xt[t] for t in self.date_list])**2\n",
    "                ).idxmax().iloc[0])\n",
    "\n",
    "        proding_list=dict(zip(\n",
    "            self.date_list,\n",
    "            [np.eye(self.l) for t in self.date_list]\n",
    "        ))\n",
    "        \n",
    "        #k>0\n",
    "        print('Estimating FM-LARS')\n",
    "        for k in trange(self.l):\n",
    "            try:\n",
    "                Zr=[]\n",
    "                Zsq_gamma=[]\n",
    "                Ak=self.Ak_list[0:k+1]\n",
    "                Ak_1=self.Ak_list[0:k]\n",
    "                for t in self.date_list:\n",
    "                    if k>0:\n",
    "                        proding_list[t]=proding_list[t]\\\n",
    "                        .dot(np.eye(self.l)-self.alpha_list[k-1]*self.zt_square[t].dot(Icrp(Ak_1,t,self.lamb)))\n",
    "                gamma_t_list=dict(zip(\n",
    "                    self.date_list,\n",
    "                    [calc_gamma_t(Ak,proding_list,t,self.lamb) for t in self.date_list]\n",
    "                ))\n",
    "                \n",
    "                for t in self.date_list:\n",
    "                    Zr.append(self.Nt[t]*proding_list[t].dot(self.xt[t]))\n",
    "                    Zsq_gamma.append(self.zt_square[t].dot(gamma_t_list[t]))\n",
    "\n",
    "                coef2=sum([Zg**2 for Zg in Zsq_gamma])\\\n",
    "                        +phi2*sum(Zsq_gamma)**2\n",
    "                coef2=coef2-coef2[self.Ak_list[0]]\n",
    "                coef1=-sum([zr * Zg for (zr,Zg) in zip(Zr,Zsq_gamma)])\\\n",
    "                        -phi2*sum(Zr)*sum(Zsq_gamma)                \n",
    "                coef1=coef1-coef1[self.Ak_list[0]]\n",
    "                cons=sum([zr**2 for zr in Zr])+phi2*sum(Zr)**2                \n",
    "                cons=cons-cons[self.Ak_list[0]]\n",
    "                \n",
    "                \n",
    "                solving_alpha_list=np.array([solve_alpha(c2,c1,c) for (c2,c1,c) in zip(coef2,coef1,cons)])\n",
    "                solving_alpha_list[Ak]=np.nan\n",
    "\n",
    "                if k<self.l-1:\n",
    "                    j_=pd.DataFrame(solving_alpha_list).idxmin().iloc[0]\n",
    "                    alpha_k=solving_alpha_list[j_]\n",
    "                else:\n",
    "                    j_=Ak[len(Ak)-1]\n",
    "                    alpha_k=1\n",
    "                self.Ak_list.append(j_)\n",
    "                self.alpha_list.append(alpha_k)\n",
    "                \n",
    "                factor_k=pd.DataFrame(np.nan,index=self.date_list,columns=self.chanames)\n",
    "                for t in self.date_list:\n",
    "                    factor_k.loc[t]=gamma_t_list[t].reshape(self.l,)\n",
    "                if k==0:\n",
    "                    self.factor_list=[self.alpha_list[k]*factor_k]\n",
    "                else:\n",
    "                    self.factor_list.append(self.factor_list[k-1]+self.alpha_list[k]*factor_k)\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        self.maxk=len(self.factor_list)\n",
    "        \n",
    "        def get_mu_hat(k):\n",
    "            mu_hat=pd.DataFrame(zip(\\\n",
    "                    [np.mean(self.factor_list[k].iloc[:,i]) for i in range(self.l)],\\\n",
    "                    [calc_NW_tvalue(self.factor_list[k].iloc[:,i]) for i in range(self.l)]\n",
    "                        ),columns=['risk premium','t value'],\\\n",
    "                       index=self.chanames).fillna(0)\n",
    "            mu_hat=mu_hat[mu_hat.iloc[:,1]!=0]\n",
    "            return(mu_hat)\n",
    "        \n",
    "        self.mu_hat=[get_mu_hat(k) for k in range(self.maxk)]\n",
    "                \n",
    "    def predict(self,Z_t,maxk=None):\n",
    "        if maxk==None:\n",
    "            maxk=self.maxk\n",
    "        prediction=[]\n",
    "        for k in range(maxk):\n",
    "            r_hat=Z_t.loc[:,self.mu_hat[k].index].dot(self.mu_hat[k].iloc[:,0].values)\n",
    "            prediction.append(r_hat)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "add71a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=FMLARS(characteristics1,ret1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7238e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▌                                                                              | 2/36 [00:00<00:02, 13.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating FM-LARS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-210090a915e7>:51: RuntimeWarning: invalid value encountered in sqrt\n",
      "  x1=(-2*coef1+np.sqrt(4*coef1**2-4*coef2*cons))/(2*coef2)\n",
      "<ipython-input-5-210090a915e7>:52: RuntimeWarning: invalid value encountered in sqrt\n",
      "  x2=(-2*coef1-np.sqrt(4*coef1**2-4*coef2*cons))/(2*coef2)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 36/36 [00:08<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "mod.fit(5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53373730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 223/223 [00:18<00:00, 11.75it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction=[]\n",
    "for i in range(mod.maxk):\n",
    "    prediction.append(copy(ret2)*0)\n",
    "    \n",
    "for t in trange(len(date2)):\n",
    "    key=date2[t]\n",
    "    pre=mod.predict(characteristics2[key])\n",
    "    for i in range(mod.maxk):\n",
    "        prediction[i].loc[key,characteristics2[key].index]=pre[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc57de1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2666052079092873"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=30\n",
    "100*(1-((ret2-prediction[k])**2).sum().sum()/(ret2**2).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff24888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
