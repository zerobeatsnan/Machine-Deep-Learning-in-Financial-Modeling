{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from AssetPricing.gru_scratch import CustomGRUromScratch, grad_clipping\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/boningzhou/fall 2023/kaggle optiver/data/optiver-trading-at-the-close/train.csv\")\n",
    "# halfway_point = len(data)//10\n",
    "# first_half_data = data.iloc[:halfway_point]\n",
    "# first_half_data.to_csv(\"small_training.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Load your CSV file\n",
    "data = pd.read_csv('/Users/boningzhou/fall 2023/kaggle optiver/data/optiver-trading-at-the-close/train.csv')\n",
    "\n",
    "# Filling missing values in 'far_price' and 'near_price' with 'reference_price'\n",
    "data['far_price'].fillna(data['reference_price'], inplace=True)\n",
    "data['near_price'].fillna(data['reference_price'], inplace=True)\n",
    "\n",
    "# Creating new binary feature based on 'seconds_in_bucket'\n",
    "data['bucket_binary'] = data['seconds_in_bucket'].apply(lambda x: 0 if x <= 290 else 1)\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "data.drop(['time_id', 'row_id'], axis=1, inplace=True)\n",
    "\n",
    "# Organizing data into sequences\n",
    "data_sorted = data.sort_values(by=['stock_id', 'date_id', 'seconds_in_bucket'])\n",
    "data_sorted = data_sorted.ffill()\n",
    "grouped = data_sorted.groupby(['stock_id', 'date_id'])\n",
    "# Selecting features and target\n",
    "features = ['imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap', 'bucket_binary']\n",
    "X = grouped[features].apply(lambda x: x.values.tolist())\n",
    "y = grouped['target'].apply(lambda x: x.values.tolist())\n",
    "\n",
    "original_lengths = X.apply(len)\n",
    "\n",
    "# Determine a common sequence length\n",
    "sequence_length = max(X.apply(len))\n",
    "\n",
    "# Padding sequences to have the same length\n",
    "X_padded = X.apply(lambda x: x[:sequence_length] if len(x) >= sequence_length else x + [[0]*len(features)]*(sequence_length-len(x)))\n",
    "y_padded = y.apply(lambda x: x[:sequence_length] if len(x) >= sequence_length else x + [0]*(sequence_length-len(x)))\n",
    "\n",
    "# Converting to numpy arrays\n",
    "X_np = np.array(X_padded.tolist())\n",
    "y_np = np.array(y_padded.tolist())\n",
    "\n",
    "original_lengths_np = np.array(original_lengths.tolist())\n",
    "\n",
    "# Save the processed data\n",
    "np.save('X_processed.npy', X_np)\n",
    "np.save('y_processed.npy', y_np)\n",
    "np.save(\"original_lengths_np.npy\", original_lengths_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the processed data (replace 'path_to_X.npy' and 'path_to_y.npy' with your file paths)\n",
    "X_np = np.load('X_processed.npy')\n",
    "y_np = np.load('y_processed.npy')\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Creating DataLoaders\n",
    "batch_size = 64  # You can adjust the batch size as needed\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input NaNs:\", torch.isnan(X_train_tensor).any())\n",
    "print(\"Target NaNs:\", torch.isnan(y_train_tensor).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import L1Loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters for the GRU model\n",
    "input_size = 12  # Number of input features\n",
    "inner_structure = []  # Structure of the inner layers\n",
    "hidden_size = 512  # Size of the hidden layer in GRU\n",
    "output_size = 1  # Number of output features (depends on your task)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomGRUromScratch(input_size, inner_structure, hidden_size, output_size, device)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss_fn = L1Loss()  # Replace with the appropriate loss function for your task\n",
    "optimizer = Adam(model.params, lr=0.001)  # Learning rate can be adjusted\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 500  # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    # model.train()\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\")\n",
    "    for X, y in train_loader_tqdm:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        state = model.begin_state(X.shape[0], device)\n",
    "        y_hat, _ = model(X, state)\n",
    "        # print(\"y_hat shape is:\",y_hat.shape)\n",
    "        # print(\"y shape is:\", y.shape)\n",
    "        y_hat_flat = y_hat.view(-1)\n",
    "        y_flat = y.view(-1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_hat_flat, y_flat)\n",
    "        total_loss += loss.item()\n",
    "        # print(loss)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # max_grad = max(param.grad.abs().max() for param in model.params if param.grad is not None)\n",
    "        # print(f\"Max gradient_before_clipping: {max_grad}\")\n",
    "\n",
    "        grad_clipping(model, 1)\n",
    "        # max_grad = max(param.grad.abs().max() for param in model.params if param.grad is not None)\n",
    "        # print(f\"Max gradient after clipping: {max_grad}\")\n",
    "\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_loss/ len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Avg. training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation step (if you have validation data)\n",
    "    # model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_val_loss = 0\n",
    "        for X_val, y_val in test_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            val_state = model.begin_state(X_val.shape[0], device)\n",
    "            y_val_hat, _ = model(X_val, val_state)\n",
    "            y_val_hat_flat = y_val_hat.view(-1)\n",
    "            y_val_flat = y_val.view(-1)\n",
    "            val_loss = loss_fn(y_val_hat_flat, y_val_flat)\n",
    "            total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        print(f\"Epoch {epoch+1}, Avg. Validation Loss: {avg_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is a regular tensor, not a packed sequence\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Take the output of the last time step for each sequence\n",
    "        last_time_step = lstm_out[:, -1, :]\n",
    "        y_pred = self.linear(last_time_step)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "input_size = 12  # Number of features in input\n",
    "hidden_size = 128  # Number of features in hidden state\n",
    "output_size = 1  # Output size\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Assuming SimpleLSTM class is already defined as per previous instructions\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "model.to(device)  # Move model to the device (CPU or GPU)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = nn.L1Loss()  # MAE Loss\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Gradient clipping value\n",
    "clip_value = 1\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        # print(loss)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(test_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "print(\"Training and evaluation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
